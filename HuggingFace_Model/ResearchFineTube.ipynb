{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('final_data.csv')\n",
    "df.rename(columns={'columns2':'text','0':'label'},inplace=True)\n",
    "df=df[['label','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# (optional) filter out any null values before creating the test, validation and training set\n",
    "#df = df[df['column_name'].notnull()]\n",
    "\n",
    "# Split dataset into training and temp (for validation and testing) - set at 15% (7.5% each)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# Split temp into validation and testing\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# setup your sets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', '__index_level_0__'],\n",
       "        num_rows: 153\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', '__index_level_0__'],\n",
       "        num_rows: 13\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', '__index_level_0__'],\n",
       "        num_rows: 14\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# look at the set\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 153/153 [00:00<00:00, 6379.77 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13/13 [00:00<00:00, 2992.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14/14 [00:00<00:00, 2715.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(dataset_dict['train']['label'])\n",
    "\n",
    "def encode_labels(example):\n",
    "    return {'encoded_label': label_encoder.transform([example['label']])[0]}\n",
    "\n",
    "for split in dataset_dict:\n",
    "    print(split)\n",
    "    dataset_dict[split] = dataset_dict[split].map(encode_labels, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"albert/albert-base-v2\"\n",
    "your_path = 'HuggingFace_Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID to Label Mapping: {0: 'NotClickBait', 1: 'clickbait'}\n",
      "Label to ID Mapping: {'NotClickBait': 0, 'clickbait': 1}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "unique_labels = sorted(list(set(dataset_dict['train']['label'])))\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "# Verify the correct labels\n",
    "print(\"ID to Label Mapping:\", config.id2label)\n",
    "print(\"Label to ID Mapping:\", config.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'albert/albert-base-v2'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "model = AlbertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Load model directly\n",
    "#from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForMaskedLM.from_pretrained(model_name,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 153/153 [00:00<00:00, 42030.95 examples/s]\n",
      "Filter: 100%|██████████| 13/13 [00:00<00:00, 7240.20 examples/s]\n",
      "Filter: 100%|██████████| 14/14 [00:00<00:00, 6059.88 examples/s]\n",
      "Map: 100%|██████████| 153/153 [00:00<00:00, 4959.30 examples/s]\n",
      "Map: 100%|██████████| 13/13 [00:00<00:00, 1704.95 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 1985.60 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', '__index_level_0__', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 153\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', '__index_level_0__', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 13\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', '__index_level_0__', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_invalid_content(example):\n",
    "    return isinstance(example['text'], str)\n",
    "\n",
    "dataset = dataset_dict.filter(filter_invalid_content, batched=False)\n",
    "\n",
    "def encode_data(batch):\n",
    "    tokenized_inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=256)\n",
    "    tokenized_inputs[\"labels\"] = batch[\"encoded_label\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "dataset_encoded = dataset.map(encode_data, batched=True)\n",
    "dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "\n",
    "def per_label_accuracy(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    correct_predictions = cm.diagonal()\n",
    "    label_totals = cm.sum(axis=1)\n",
    "    per_label_acc = np.divide(correct_predictions, label_totals, out=np.zeros_like(correct_predictions, dtype=float), where=label_totals != 0)\n",
    "    return dict(zip(labels, per_label_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    decoded_labels = label_encoder.inverse_transform(labels)\n",
    "    decoded_preds = label_encoder.inverse_transform(preds)\n",
    "\n",
    "    precision = precision_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    recall = recall_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    f1 = f1_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    acc = accuracy_score(decoded_labels, decoded_preds)\n",
    "\n",
    "    labels_list = list(label_encoder.classes_)\n",
    "    per_label_acc = per_label_accuracy(decoded_labels, decoded_preds, labels_list)\n",
    "\n",
    "    per_label_acc_metrics = {}\n",
    "    for label, accuracy in per_label_acc.items():\n",
    "        label_key = f\"accuracy_label_{label}\"\n",
    "        per_label_acc_metrics[label_key] = accuracy\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        **per_label_acc_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=0.7712101777394612, metrics={'train_runtime': 42.5952, 'train_samples_per_second': 10.776, 'train_steps_per_second': 0.352, 'total_flos': 578455420140.0, 'train_loss': 0.7712101777394612, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=your_path,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=1000,\n",
    "    gradient_accumulation_steps=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_encoded['train'],\n",
    "    eval_dataset=dataset_encoded['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NotClickBait', 'clickbait'], dtype='<U12')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_name='myclickbaitmodelv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "trainer.save_model(my_model_name)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('text-classification', model=my_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Controversial Truth about Tech Debt\n",
      "[{'label': 'clickbait', 'score': 0.5173347592353821}]\n",
      "Output: clickbait\n",
      "Title: A Comprehensive Guide for Getting Started with Hugging Face\n",
      "[{'label': 'clickbait', 'score': 0.5692624449729919}]\n",
      "Output: clickbait\n",
      "Title: OpenAI GPT-4o: The New Best AI Model in the World. Like in the Movies. For Free\n",
      "[{'label': 'NotClickBait', 'score': 0.538385808467865}]\n",
      "Output: NotClickBait\n",
      "Title: GPT4 Omni — So much more than just a voice assistant\n",
      "[{'label': 'clickbait', 'score': 0.7242204546928406}]\n",
      "Output: clickbait\n",
      "Title: Building Vector Databases with FastAPI and ChromaDB\n",
      "[{'label': 'clickbait', 'score': 0.6546621918678284}]\n",
      "Output: clickbait\n",
      "Title: How Pieter Levels Makes (At Least) $210K a Month From His Laptop — With Zero Employees\n",
      "[{'label': 'clickbait', 'score': 0.6482016444206238}]\n",
      "Output: clickbait\n",
      "Title: Which Is Better: Teachers or AI in the Classroom?\n",
      "[{'label': 'clickbait', 'score': 0.7300415635108948}]\n",
      "Output: clickbait\n",
      "Title: How to Build Enterprise-Scale Generative AI Agents with AWS Bedrock: A Comprehensive Guide\n",
      "[{'label': 'clickbait', 'score': 0.7518175840377808}]\n",
      "Output: clickbait\n",
      "Title: The Best Way To Start Your One-Person Business\n",
      "[{'label': 'clickbait', 'score': 0.5945402383804321}]\n",
      "Output: clickbait\n",
      "Title: How to earn one crore in 1 days by following these 2 steps\n",
      "[{'label': 'clickbait', 'score': 0.5919113755226135}]\n",
      "Output: clickbait\n"
     ]
    }
   ],
   "source": [
    "example_titles = [\n",
    "    \"The Controversial Truth about Tech Debt\",\n",
    "    \"A Comprehensive Guide for Getting Started with Hugging Face\",\n",
    "    \"OpenAI GPT-4o: The New Best AI Model in the World. Like in the Movies. For Free\",\n",
    "    \"GPT4 Omni — So much more than just a voice assistant\",\n",
    "    \"Building Vector Databases with FastAPI and ChromaDB\",\n",
    "    \"How Pieter Levels Makes (At Least) $210K a Month From His Laptop — With Zero Employees\",\n",
    "    \"Which Is Better: Teachers or AI in the Classroom?\",\n",
    "    \"How to Build Enterprise-Scale Generative AI Agents with AWS Bedrock: A Comprehensive Guide\",\n",
    "    \"The Best Way To Start Your One-Person Business\",\n",
    "    \"How to earn one crore in 1 days by following these 2 steps\",\n",
    "]\n",
    "\n",
    "for title in example_titles:\n",
    "    result = pipe(title)\n",
    "    print(f\"Title: {title}\")\n",
    "    print(result)\n",
    "    print(f\"Output: {result[0]['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
